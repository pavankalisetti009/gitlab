# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ResolveVulnerability < Gitlab::Llm::Completions::Base
        include Gitlab::Llm::Completions::ResolveVulnerability::Helpers

        EmptyResponseError = Class.new(StandardError)

        def execute
          ai_response, diff_extracted, description_options = response_for(user, vulnerability, @options)
          response = if diff_extracted
                       if @options[:suggestion_merge_request_id]
                         create_merge_request_suggestion(user,
                           vulnerability,
                           merge_request_gid(@options[:suggestion_merge_request_id]),
                           ai_response,
                           description_options)
                       else
                         create_merge_request(user, vulnerability, ai_response, description_options)
                       end
                     else
                       ai_response
                     end

          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        rescue StandardError => error
          Gitlab::ErrorTracking.track_exception(error)
          log_local_error(error)

          response = formatted_error_response(error_message(error))
          response_modifier = modify_response(response, vulnerability)
          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(user, vulnerability), expires_in: 10.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt
            ai_response = request(user, prompt)

            extract_llm_change(ai_response)
          end
        end

        private

        def invalidate_cache!
          Rails.cache.delete(cache_key(user, vulnerability))
        end
      end
    end
  end
end
