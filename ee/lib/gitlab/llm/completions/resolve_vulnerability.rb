# frozen_string_literal: true

module Gitlab
  module Llm
    module Completions
      class ResolveVulnerability < Gitlab::Llm::Completions::Base
        include Gitlab::Llm::Completions::ResolveVulnerability::Helpers
        include Gitlab::InternalEventsTracking

        EmptyResponseError = Class.new(StandardError)

        def execute
          ai_response, diff_extracted, description_options = response_for(user, vulnerability, @options)

          vulnerable_merge_request_id = merge_request_gid(@options[:vulnerable_merge_request_id])
          response = if diff_extracted
                       create_merge_request_with_resolution(
                         user,
                         vulnerability,
                         ai_response,
                         description_options,
                         vulnerable_merge_request_id: vulnerable_merge_request_id
                       )

                       # TODO: I think the note creation logic can be
                       # pulled all the way to this level
                       #
                       # Something like:
                       #
                       #    attach_note(mr) if vulnerable_merge_request_id
                     else
                       ai_response
                     end

          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        rescue Templates::Vulnerabilities::CodeLengthError => error
          handle_warn(error, 'error_building_request')
        rescue EmptyResponseError => error
          handle_warn(error, 'error_response_received')
        rescue StandardError => error
          handle_error(error)
        end

        def handle_warn(error, event_name)
          log_warn(
            message: "LLM completion error",
            event_name: event_name,
            ai_component: 'resolve_vulnerability',
            error_message: error.to_s
          )

          error_response(error)
        end

        def response_for(user, vulnerability, options)
          Rails.cache.fetch(cache_key(user, vulnerability), expires_in: 10.minutes, skip_nil: true) do
            prompt = ai_prompt_class.new(vulnerability, options).to_prompt
            ai_response = request(user, prompt)

            extract_llm_change(ai_response)
          end
        end

        private

        def handle_error(error)
          Gitlab::ErrorTracking.track_exception(error)
          log_local_error(error)
          error_response(error)
        end

        def error_response(error)
          response = formatted_error_response(error_message(error))
          response_modifier = modify_response(response, vulnerability)

          ::Gitlab::Llm::GraphqlSubscriptionResponseService.new(
            user, vulnerability, response_modifier, options: response_options
          ).execute

          response_modifier
        end

        def invalidate_cache!
          Rails.cache.delete(cache_key(user, vulnerability))
        end
      end
    end
  end
end
