# frozen_string_literal: true

require 'spec_helper'

RSpec.describe Gitlab::Llm::Completions::ResolveVulnerability, feature_category: :vulnerability_management do
  let(:prompt_class) { Gitlab::Llm::Templates::Vulnerabilities::ResolveVulnerability }
  let(:merge_request_service) { ::MergeRequests::CreateFromVulnerabilityDataService }
  let(:merge_request) { create(:merge_request, source_project: project) }
  let(:mr_url) { Gitlab::Routing.url_helpers.project_merge_request_url(project, merge_request) }
  let(:finding_location_file) { 'main.c' }
  let(:description_options) { {} }
  let(:changed_code) { "somecode\nexecute" }
  let(:content) { "```\n#{changed_code}\n``` and a ```\nsecond code block\n```" }
  let(:example_response) do
    {
      "predictions" => [
        {
          "content" => content,
          "safetyAttributes" => {
            "categories" => ["Violent"],
            "scores" => [0.4000000059604645],
            "blocked" => false
          }
        }
      ],
      "deployedModelId" => "1",
      "model" => "projects/1/locations/us-central1/models/codechat-bison-001",
      "modelDisplayName" => "codechat-bison-001",
      "modelVersionId" => "1"
    }
  end

  let(:example_response_anthropic) do
    {
      "id" => "msg_01JPMpeAAtZdoNLvq8Nqhd3D",
      "type" => "message",
      "role" => "assistant",
      "model" => "claude-3-haiku-20240307",
      "content" => [
        {
          "type" => "text",
          "text" => anthropic_content
        }
      ],
      "stop_reason" => "end_turn",
      "stop_sequence" => nil,
      "usage" => {
        "input_tokens" => 130,
        "output_tokens" => 26
      }
    }
  end

  let(:error_response) do
    { 'error' => { 'message' => 'Ooops...' } }
  end

  let(:errors) do
    ["Ooops..."]
  end

  let_it_be(:user) { create(:user) }
  let_it_be(:user2) { create(:user) }
  let_it_be(:project) do
    create(:project, :custom_repo, files: {
                                     'main.c' => "#include <stdio.h>\n\nint main() { printf(\"hello, world!\"); }"
                                   },
      developers: [user, user2])
  end

  let(:vulnerability) { create(:vulnerability, :with_finding, project: project) }

  let(:prompt_message) do
    build(:ai_message, :resolve_vulnerability, user: user, resource: vulnerability, request_id: 'uuid')
  end

  let(:options) { {} }

  let(:logger) { instance_double('Gitlab::Llm::Logger') }

  subject(:resolve) { described_class.new(prompt_message, prompt_class, options) }

  def execute_resolve(message_params = {}, options = {})
    message = build(:ai_message, :resolve_vulnerability,
      { user: user, resource: vulnerability, request_id: 'uuid' }.merge(message_params))

    described_class.new(message, prompt_class, options).execute
  end

  before do
    stub_licensed_features(security_dashboard: true)

    [:admin_all_resources, :resolve_vulnerability_with_ai].each do |permission|
      allow(user).to receive(:can?).with(permission).and_return(true)
      allow(user2).to receive(:can?).with(permission).and_return(true)
    end
    allow(GraphqlTriggers).to receive(:ai_completion_response)
    vulnerability.finding.location['file'] = finding_location_file
    vulnerability.finding.location['start_line'] = 1
    allow(Gitlab::Llm::Logger).to receive(:build).and_return(logger)
    allow(logger).to receive(:error)
  end

  shared_examples 'resolve vulnerability completions' do |llm_client, client_method|
    context 'when the client returns an unsuccessful response' do
      before do
        allow_next_instance_of(llm_client) do |client|
          allow(client).to receive(client_method).and_return(
            error_response
          )
        end
      end

      it 'publishes the error to the graphql subscription' do
        resolve.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response)
          .with(an_object_having_attributes(
            user: user,
            resource: vulnerability,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: errors
          ))
      end
    end

    context 'when there is no file for the finding in the repo' do
      let(:finding_location_file) { 'no_such_file.c' }

      it 'returns an error' do
        resolve.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response)
          .with(an_object_having_attributes(
            user: user,
            resource: vulnerability,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: ['Unable to locate source code for vulnerability']
          ))
      end
    end

    context 'when vulnerable code exceeds maximum length' do
      it 'returns an error' do
        allow_next_instance_of(Gitlab::Llm::Templates::Vulnerabilities::ResolveVulnerability) do |instance|
          allow(instance).to receive(:max_code_length).and_return(0)
        end

        resolve.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response)
          .with(an_object_having_attributes(
            user: user,
            resource: vulnerability,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: ["Vulnerable code exceeds maximum length (0)"]
          ))
      end
    end

    context 'when vulnerability report_type is secret detection' do
      it 'returns an error' do
        allow(vulnerability).to receive(:secret_detection?).and_return(true)

        resolve.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response)
          .with(an_object_having_attributes(
            user: user,
            resource: vulnerability,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: ["Refusing to send possible secrets in AI prompt"]
          ))
      end
    end

    context 'when the client returns a successful response' do
      before do
        allow(llm_client).to receive(:new).and_call_original
        allow_next_instance_of(llm_client) do |client|
          allow(client).to receive(client_method).and_return(example_response)
        end

        allow(merge_request_service).to receive(:new).and_call_original
        allow_next_instance_of(merge_request_service) do |mr_service|
          allow(mr_service).to receive(:execute).and_return(
            { merge_request: merge_request, status: :success }
          )
        end
      end

      it 'requests that a MR be created with the extracted patch' do
        resolve.execute

        expect(merge_request_service).to have_received(:new).with(
          project,
          vulnerability,
          user,
          llm_patch: code_patch,
          description_options: description_options
        )
      end

      it 'publishes the created merge request for the fix' do
        resolve.execute

        expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
          an_object_having_attributes(
            content: mr_url,
            role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
            request_id: 'uuid',
            errors: [],
            user: user,
            resource: vulnerability
          )
        )
      end

      context 'when the LLM responds with a typed code block' do
        let(:content) { "```java\n#{code_patch}\n```" }

        it 'publishes the created merge request for the fix' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              content: mr_url,
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [],
              user: user,
              resource: vulnerability
            )
          )
        end
      end

      context 'when an unexpected error is raised' do
        let(:error) { StandardError.new("Ooops...") }

        before do
          allow_next_instance_of(llm_client) do |client|
            allow(client).to receive(client_method).and_raise(error)
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'records the error' do
          resolve.execute
          expect(Gitlab::ErrorTracking).to have_received(:track_exception).with(error)
          expect(logger).to have_received(:error).with(message: "LLM completion error", error: error.to_s,
            source: "Gitlab::Llm::Completions::ResolveVulnerability")
        end

        it 'publishes a generic error to the graphql subscription' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::DEFAULT_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the client experiences a Net::ReadTimeout' do
        let(:error) { Net::ReadTimeout.new }

        before do
          allow_next_instance_of(llm_client) do |client|
            allow(client).to receive(client_method).and_raise(error)
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'records the error' do
          resolve.execute
          expect(Gitlab::ErrorTracking).to have_received(:track_exception).with(error)
          expect(logger).to have_received(:error).with(message: "LLM completion error", error: "Net::ReadTimeout",
            source: "Gitlab::Llm::Completions::ResolveVulnerability")
        end

        it 'publishes a generic error to the graphql subscription' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::CLIENT_TIMEOUT_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the client raises a Gitlab::AiGateway::ForbiddenError' do
        let(:error) { Gitlab::AiGateway::ForbiddenError.new }

        before do
          allow_next_instance_of(llm_client) do |client|
            allow(client).to receive(client_method).and_raise(error)
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'records the error' do
          resolve.execute
          expect(Gitlab::ErrorTracking).to have_received(:track_exception).with(error)
          expect(logger).to have_received(:error).with(message: "LLM completion error",
            error: "Gitlab::AiGateway::ForbiddenError",
            source: "Gitlab::Llm::Completions::ResolveVulnerability")
        end

        it 'publishes a generic error to the graphql subscription' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::FORBIDDEN_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the CreateFromVulnerabilityDataService service fails to create an MR' do
        before do
          allow_next_instance_of(::MergeRequests::CreateFromVulnerabilityDataService) do |service|
            allow(service).to receive(:execute).and_return({ status: :error })
          end
          allow(Gitlab::ErrorTracking).to receive(:track_exception)
        end

        it 'publishes a resolution error to the graphql subscription' do
          resolve.execute

          expect(logger).to have_received(:error).with(message: "LLM completion error",
            error: described_class::RESOLUTION_FAILURE_ERROR,
            source: "Gitlab::Llm::Completions::ResolveVulnerability")

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::RESOLUTION_FAILURE_ERROR],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the response is malformed' do
        before do
          allow_next_instance_of(::MergeRequests::CreateFromVulnerabilityDataService) do |service|
            allow(service).to receive(:execute)
                                .and_return({ status: :error, unreadable_response: true })
          end
        end

        it 'publishes a resolution error to the graphql subscription' do
          resolve.execute

          expect(logger).to have_received(:error).with(message: "LLM completion error",
            error: described_class::RESPONSE_FAILURE_ERROR,
            source: "Gitlab::Llm::Completions::ResolveVulnerability")

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [described_class::RESPONSE_FAILURE_ERROR],
              user: user,
              resource: vulnerability
            ))
        end

        it 'clears cache' do
          resolve.execute

          expect(Rails.cache.read([user.id, vulnerability.cache_key, 'resolve'].join('/'))).to be_nil
        end
      end

      context 'when request is cached', :use_clean_rails_redis_caching do
        context 'when unique users make the same request' do
          let(:fake_client) { instance_double(llm_client) }

          before do
            allow(llm_client).to receive(:new).and_return(fake_client)
            allow(fake_client).to receive(client_method).twice.and_return(example_response)
          end

          it 'makes a fresh request for each user' do
            # cache miss
            execute_resolve
            execute_resolve({ user: user2 })

            # cache hit
            execute_resolve
            execute_resolve({ user: user2 })

            expect(fake_client).to have_received(client_method).exactly(2).times
          end
        end
      end
    end
  end

  describe '#execute', :clean_gitlab_redis_cache do
    let(:tracking_context) { { request_id: "uuid", action: :resolve_vulnerability } }

    it_behaves_like 'resolve vulnerability completions', ::Gitlab::Llm::ResolveVulnerability::Client,
      :messages_complete do
      let(:code_patch) { "<old_code>printf(\"hello, world!\");</old_code><new_code>\n#{changed_code}</new_code>" }
      let(:is_false_positive) { "false" }
      let(:anthropic_content) do
        <<~CONTENT
          <analysis>
          analysis</analysis>
          <fixed_code>
          #{code_patch}</fixed_code>
          <is_false_positive>
          #{is_false_positive}</is_false_positive>
          <summary>
          some summary</summary>
        CONTENT
      end

      let(:example_response) { example_response_anthropic }
      let(:description_options) { { analysis_data: "analysis", summary_data: "some summary" } }

      context 'when the LLM responds with an empty code block' do
        let(:code_patch) { '' }

        before do
          allow(::Gitlab::Llm::ResolveVulnerability::Client).to receive(:new).and_call_original
          allow_next_instance_of(::Gitlab::Llm::ResolveVulnerability::Client) do |client|
            allow(client).to receive(:messages_complete).and_return(example_response_anthropic)
          end
        end

        it 'publishes a false positive error to the graphql subscription' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [a_string_starting_with('The response from the AI provider was ' \
                'empty because it has determined this vulnerability to be a false positive.')],
              user: user,
              resource: vulnerability
            ))
        end
      end

      context 'when the LLM responds with the is_false_positive tag' do
        let(:is_false_positive) { "true" }

        before do
          allow(::Gitlab::Llm::ResolveVulnerability::Client).to receive(:new).and_call_original
          allow_next_instance_of(::Gitlab::Llm::ResolveVulnerability::Client) do |client|
            allow(client).to receive(:messages_complete).and_return(example_response_anthropic)
          end
        end

        it 'publishes a false positive error to the graphql subscription' do
          resolve.execute

          expect(GraphqlTriggers).to have_received(:ai_completion_response).with(
            an_object_having_attributes(
              role: ::Gitlab::Llm::AiMessage::ROLE_ASSISTANT,
              request_id: 'uuid',
              errors: [a_string_starting_with('The response from the AI provider was ' \
                'empty because it has determined this vulnerability to be a false positive.')],
              user: user,
              resource: vulnerability
            ))
        end
      end
    end
  end
end
